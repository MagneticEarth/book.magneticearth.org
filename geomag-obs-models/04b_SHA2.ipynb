{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Harmonic Models 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notebook dependencies\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from src import sha, lsfile, cfile, IGRF13_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more tangible example of modelling using spherical harmonics\n",
    "\n",
    "In this notebook we compute spherical harmonic models from an input data set representing a very simple world map with land encoded as 1 and water as 0 (so the data takes the values 0 and 1 only). We investigate the ability of models with different choices of maximum spherical harmonic degree to represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lsdata = np.genfromtxt(lsfile, delimiter=',')\n",
    "\n",
    "x = lsdata[:,0].reshape(36, 72)\n",
    "y = lsdata[:,1].reshape(36,72)\n",
    "z = lsdata[:,2].reshape(36,72)\n",
    "\n",
    "# Plot them\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.contourf(x,y,z)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>> USER INPUT HERE: Set the maximum spherical harmonic degree of the analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 10 # Max degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are used for the spherical harmonic analysis and synthesis steps\n",
    "# Firstly compute a spherical harmonic model of degree and order nmax using the input data as plotted above\n",
    "def sh_analysis(lsdata, nmax):\n",
    "    npar = (nmax+1)*(nmax+1)\n",
    "    ndat = len(lsdata)\n",
    "    lhs  = np.zeros(npar*ndat).reshape(ndat,npar)\n",
    "\n",
    "    rhs  = np.zeros(ndat)\n",
    "    line = np.zeros(npar)\n",
    "    ic   = -1\n",
    "    for i in range(ndat):\n",
    "        th  = 90 - lsdata[i][1]\n",
    "        ph  = lsdata[i][0] \n",
    "        rhs[i] = lsdata[i][2]\n",
    "        cmphi, smphi = sha.csmphi(nmax,ph)\n",
    "        pnm = sha.pnm_calc(nmax, th)\n",
    "        for n in range(nmax+1):\n",
    "            igx = sha.gnmindex(n,0)\n",
    "            ipx = sha.pnmindex(n,0)\n",
    "            line[igx] = pnm[ipx]\n",
    "            for m in range(1,n+1):\n",
    "                igx = sha.gnmindex(n,m)\n",
    "                ihx = sha.hnmindex(n,m)\n",
    "                ipx = sha.pnmindex(n,m)\n",
    "                line[igx] = pnm[ipx]*cmphi[m]\n",
    "                line[ihx] = pnm[ipx]*smphi[m]\n",
    "        lhs[i,:] = line\n",
    "\n",
    "    shmod  = np.linalg.lstsq(lhs, rhs.reshape(len(lsdata),1), rcond=None)\n",
    "    return(shmod)\n",
    "\n",
    "\n",
    "# Now use the model to synthesise values on a 5 degree grid in latitude and longitude\n",
    "def sh_synthesis(shcofs, nmax):\n",
    "    newdata =np.zeros(72*36*3).reshape(2592,3)\n",
    "    ic = 0\n",
    "    for ilat in range(36):\n",
    "        delta = 5*ilat+2.5\n",
    "        lat = 90 - delta\n",
    "        for iclt in range(72):\n",
    "            corr  = 5*iclt+2.5\n",
    "            long  = -180+corr\n",
    "            colat = 90-lat\n",
    "            cmphi, smphi = sha.csmphi(nmax,long)\n",
    "            vals  = np.dot(sha.gh_phi(shcofs, nmax, cmphi, smphi), sha.pnm_calc(nmax, colat))\n",
    "            newdata[ic,0]=long\n",
    "            newdata[ic,1]=lat\n",
    "            newdata[ic,2]=vals\n",
    "            ic += 1\n",
    "    return(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the spherical harmonic coefficients\n",
    "shmod = sh_analysis(lsdata=lsdata, nmax=nmax)\n",
    "\n",
    "# Read the model coefficients\n",
    "shcofs = shmod[0]\n",
    "\n",
    "# Synthesise the model coefficients on a 5 degree grid\n",
    "newdata = sh_synthesis(shcofs=shcofs, nmax=nmax)\n",
    "# Reshape for plotting purposes\n",
    "x = newdata[:,0].reshape(36, 72)\n",
    "y = newdata[:,1].reshape(36,72)\n",
    "z = newdata[:,2].reshape(36,72)\n",
    "\n",
    "# Plot the results\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "levels = [-1.5, -0.75, 0, 0.25, 0.5, 0.75, 2.]\n",
    "plt.contourf(x,y,z)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spherical harmonic models with data gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the data set is incomplete? In this section, you can experiment by removing data within a spherical cap of specified radius and pole position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greatcircle(th1, ph1, th2, ph2):\n",
    "    th1 = np.deg2rad(th1)\n",
    "    th2 = np.deg2rad(th2)\n",
    "    dph = np.deg2rad(dlong(ph1,ph2))\n",
    "\n",
    "  # Apply the cosine rule of spherical trigonometry\n",
    "    dth = np.arccos(np.cos(th1)*np.cos(th2) + \\\n",
    "              np.sin(th1)*np.sin(th2)*np.cos(dph))\n",
    "    return(dth)\n",
    "\n",
    "def dlong (ph1, ph2):\n",
    "    ph1 = np.sign(ph1)*abs(ph1)%360   # These lines return a number in the \n",
    "    ph2 = np.sign(ph2)*abs(ph2)%360   # range -360 to 360\n",
    "    if(ph1 < 0): ph1 = ph1 + 360      # Put the results in the range 0-360\n",
    "    if(ph2 < 0): ph2 = ph2 + 360\n",
    "    dph = max(ph1,ph2) - min(ph1,ph2) # So the answer is positive and in the\n",
    "                                      # range 0-360\n",
    "    if(dph > 180): dph = 360-dph      # So the 'short route' is returned\n",
    "    return(dph)\n",
    "\n",
    "def gh_phi(gh, nmax, cp, sp):\n",
    "    rx = np.zeros(nmax*(nmax+3)//2+1)\n",
    "    igx=-1\n",
    "    igh=-1\n",
    "    for i in range(nmax+1):\n",
    "        igx += 1\n",
    "        igh += 1\n",
    "        rx[igx]= gh[igh]\n",
    "        for j in range(1,i+1):\n",
    "            igh += 2\n",
    "            igx += 1\n",
    "            rx[igx] = (gh[igh-1]*cp[j] + gh[igh]*sp[j])\n",
    "    return(rx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>> USER INPUT HERE: Set the location and size of the data gap**\n",
    "\n",
    "Enter the pole colatitude and longitude in degrees and the radius in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a section of data centered on colat0, long0 and radius='radius':\n",
    "colat0  = 113\n",
    "long0   = 135\n",
    "radius  = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdata_gap = lsdata.copy()\n",
    "for row in range(len(lsdata_gap)):\n",
    "    colat = 90 - lsdata_gap[row,1]\n",
    "    long  = lsdata_gap[row,0]\n",
    "    if greatcircle(colat, long, colat0, long0) < radius/6371.2:\n",
    "        lsdata_gap[row,2] = np.nan\n",
    "\n",
    "print('Blanked out: ', np.count_nonzero(np.isnan(lsdata_gap)))\n",
    "\n",
    "x_gap = lsdata_gap[:,0].reshape(36, 72)\n",
    "y_gap = lsdata_gap[:,1].reshape(36,72)\n",
    "z_gap = lsdata_gap[:,2].reshape(36,72)\n",
    "\n",
    "# Plot the map with omitted data\n",
    "plt.contourf(x_gap, y_gap, z_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USER INPUT HERE: Set the maximum spherical harmonic degree of the analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 12 #Max degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the non-nan data\n",
    "lsdata_gap = lsdata_gap[~np.isnan(lsdata_gap[:,2])]\n",
    "\n",
    "# Obtain the spherical harmonic coefficients for the incomplete data set\n",
    "shmod = sh_analysis(lsdata=lsdata_gap, nmax=nmax)\n",
    "\n",
    "# Read the model coefficients\n",
    "shcofs = shmod[0]\n",
    "\n",
    "# Synthesise the model coefficients on a 5 degree grid\n",
    "newdata = sh_synthesis(shcofs=shcofs, nmax=nmax)\n",
    "# Reshape for plotting purposes\n",
    "x_new = newdata[:,0].reshape(36, 72)\n",
    "y_new = newdata[:,1].reshape(36,72)\n",
    "z_new = newdata[:,2].reshape(36,72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum and minimum of the synthesised data. How do they compare to the original data, which was composed of only ones and zeroes? How do the max/min change as you vary the data gap size and the analysis resolution? Why? Try this with a larger gap, e.g. 5000 km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.min(z_new))\n",
    "print(np.max(z_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results with colour scale according to the data\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.contourf(x_new, y_new, z_new)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what happens when we restrict the colour scale to values between 0 and 1 (so values <0 and >1 are compressed into opposite ends of the colour scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "levels = [0, 0.25, 0.5, 0.75, 1.]\n",
    "plt.contourf(x_new, y_new, z_new, levels, extend='both')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a spherical harmonic model of the geomagnetic field\n",
    "\n",
    "There is a file containing virtual observatory (VO) data calculated from the Swarm 3-satellite constellation in this repository. VOs use a local method to provide point estimates of the magnetic field at a specified time and fixed location at satellite altitude. This technique has various benefits, including: \n",
    "\n",
    "1. Easier comparison between satellite (moving instrument) and ground observatory (fixed instrument) data, which is particularly useful when studying time changes of the magnetic field, e.g. geomagnetic jerks.\n",
    "2. One can compute VOs on a regular spatial grid to mitigate the effects of uneven ground observatory coverage.\n",
    "\n",
    "A brief summary of the method used to calculate each of these VO data points:\n",
    "\n",
    "1. Swarm track data over four months are divided into 300 globally distributed equal area bins\n",
    "2. Gross outliers (deviating over 500nT from the CHAOS-6-x7 internal field model) are removed\n",
    "3. Only data from magnetically quiet, dark times are kept\n",
    "4. Estimates of the core and crustal fields from CHAOS-6-x7 are subtracted from each datum to give the residuals\n",
    "5. A local magnetic potential $V$ is fit to the residuals within the cylinder\n",
    "6. Values of the residual magnetic field in spherical coordinates ($r$, $\\theta$, $\\phi$) are computed from the obtained magnetic potential using $B=-\\nabla V$ at centre of the bin (at 490km altitude)\n",
    "7.\tAn estimate of the modelled core field at the VO calculation point is added back onto the residual field to give the total internal magnetic field value\n",
    "\n",
    "In this section, we use VO data centred on 2015.0 to produce a degree 13 spherical harmonic model of the geomagnetic field and compare our (satellite data only) model to the IGRF (computed using both ground and satellite data) at 2015.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the VO data\n",
    "cols  = ['Year','Colat','Long','Rad','Br','Bt', 'Bp']\n",
    "bvals = pd.read_csv(cfile, sep='\\s+', skiprows=0, header=None, index_col=0,\n",
    "                    na_values=[99999.00,99999.00000], names=cols, comment='%')\n",
    "bvals = bvals.dropna()\n",
    "bvals['Bt']= -bvals['Bt']\n",
    "bvals['Br']= -bvals['Br']\n",
    "colnames=['Colat','Long','Rad','Bt','Bp', 'Br']\n",
    "bvals=bvals.reindex(columns=colnames)\n",
    "bvals.columns = ['Colat','Long','Rad','X','Y','Z']\n",
    "print(bvals.info())\n",
    "# Set the date to 2015.0 (this is the only common date between the VO data file and the IGRF coefficients file)\n",
    "epoch = 2015.0\n",
    "# Set the model resolution to 13 (the same as IGRF)\n",
    "nmax = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geomagnetic_model(epoch, nmax, data, RREF=6371.2):\n",
    "    # Select the data for that date\n",
    "    colat =  np.array(data.loc[epoch]['Colat'])\n",
    "    nx   = len(colat) # Number of data triples\n",
    "    ndat = 3*nx    # Total number of data\n",
    "    elong =  np.array(data.loc[epoch]['Long'])\n",
    "    rad   =  np.array(data.loc[epoch]['Rad'])\n",
    "    rhs   =  data.loc[epoch][['X', 'Y', 'Z']].values.reshape(ndat,1)\n",
    "    npar = (nmax+1)*(nmax+1) # Number of model parameters\n",
    "    nx   = len(colat) # Number of data triples\n",
    "    ndat = 3*nx    # Total number of data\n",
    "    print('Number of data triples selected =', nx)\n",
    "\n",
    "    # Arrays for the equations of condition\n",
    "    lhs  = np.zeros(npar*ndat).reshape(ndat,npar)\n",
    "    linex = np.zeros(npar); liney = np.zeros(npar); linez = np.zeros(npar)\n",
    "\n",
    "    iln = 0 # Row counter\n",
    "    for i in range(nx): # Loop over data triplets (X, Y, Z)\n",
    "        r  = rad[i]\n",
    "        th = colat[i]; ph  = elong[i] \n",
    "        rpow = sha.rad_powers(nmax, RREF, r)\n",
    "        cmphi, smphi = sha.csmphi(nmax, ph)\n",
    "        pnm, xnm, ynm, znm = sha.pxyznm_calc(nmax, th)\n",
    "\n",
    "        for n in range(nmax+1): # m=0\n",
    "            igx = sha.gnmindex(n,0) # Index for g(n,0) in gh conventional order\n",
    "            ipx = sha.pnmindex(n,0) # Index for pnm(n,0) in array pnm\n",
    "            rfc = rpow[n]\n",
    "            linex[igx] =  xnm[ipx]*rfc\n",
    "            liney[igx] =  0\n",
    "            linez[igx] = znm[ipx]*rfc\n",
    "\n",
    "            for m in range(1,n+1): # m>0\n",
    "                igx = sha.gnmindex(n,m) # Index for g(n,m)\n",
    "                ihx = igx + 1           # Index for h(n,m)\n",
    "                ipx = sha.pnmindex(n,m) # Index for pnm(n,m)\n",
    "                cpr = cmphi[m]*rfc\n",
    "                spr = smphi[m]*rfc\n",
    "                linex[igx] =  xnm[ipx]*cpr; linex[ihx] =  xnm[ipx]*spr\n",
    "                liney[igx] =  ynm[ipx]*spr; liney[ihx] = -ynm[ipx]*cpr\n",
    "                linez[igx] =  znm[ipx]*cpr; linez[ihx] =  znm[ipx]*spr\n",
    "\n",
    "        lhs[iln,  :] = linex\n",
    "        lhs[iln+1,:] = liney\n",
    "        lhs[iln+2 :] = linez\n",
    "        iln += 3\n",
    "\n",
    "    shmod = np.linalg.lstsq(lhs, rhs, rcond=None) # Include the monopole\n",
    "    # shmod = np.linalg.lstsq(lhs[:,1:], rhs, rcond=None) # Exclude the monopole\n",
    "    gh = shmod[0]\n",
    "    return(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = geomagnetic_model(epoch=epoch, nmax=nmax, data=bvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the IGRF coefficients for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igrf13 = pd.read_csv(IGRF13_FILE, delim_whitespace=True,  header=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igrf13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 2015 IGRF coefficients\n",
    "igrf = np.array(igrf13[str(epoch)])\n",
    "# Insert a zero in place of the monopole coefficient (this term is NOT included in the IRGF, or any other field model)\n",
    "igrf = np.insert(igrf, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our model coefficients to IGRF13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"VO only model \\t   IGRF13\")\n",
    "for i in range(196):\n",
    "    print(\"% 9.1f\\t%9.1f\" %(coeffs[i], igrf[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Above we took a simple approach and computed a model for 2015.0 uding VO data for 2015.0. Use VOs to produce a model at a time of your choice between 2014 and 2018 (other than 2015.0 as we did above) and compare your coefficients to the IGRF at that time. **Hints:** For dates other than yyyy.0, you will need to interpolate the IGRF coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a second spherical harmonic model based on ground observatory data only and compare it to your satellite data only model and to the IGRF. To do this, you can use observatory annual means as the input data, which we have supplied in the file `oamjumpsapplied.dat` in the external data directory. **Hint:** The `mag` module contains a function that will extract all annual mean values for all observatories in a given year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pooch (not currently in VRE)\n",
    "import sys\n",
    "!{sys.executable} -m pip install pooch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data and import the relevant function\n",
    "import pooch\n",
    "\n",
    "oamjumpsapplied = pooch.retrieve(\n",
    "    \"https://raw.githubusercontent.com/MagneticEarth/IAGA_SummerSchool2019/master/data/external/oamjumpsapplied.dat\",\n",
    "    known_hash=\"dee77ded79cb0d86e3366a550830cad9de741322212faf9a74f3d03cb6bdd872\"\n",
    ")\n",
    "\n",
    "from src.mag import obs_ann_means_one_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = obs_ann_means_one_year(2000, oamjumpsapplied)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "The land/sea data file was provided by John Stevenson (BGS).\n",
    "The VO data were computed by Magnus Hammer (DTU) and the file prepared by Will Brown (BGS)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
